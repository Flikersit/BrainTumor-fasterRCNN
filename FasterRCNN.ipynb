{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_regressions_pred_to_anchors_or_proposals(box_trans, anchor_or_prop):\n",
    "        box_trans = box_trans.reshape(\n",
    "            box_trans.size(0), -1, 4\n",
    "        )\n",
    "        w = anchor_or_prop[:, 2] - anchor_or_prop[:, 0]\n",
    "        h = anchor_or_prop[:, 3] - anchor_or_prop[:, 1]\n",
    "        center_x = anchor_or_prop[:, 0] + 0.5*w\n",
    "        center_y = anchor_or_prop[:, 1] + 0.5*h\n",
    "\n",
    "        dx = box_trans[..., 0]\n",
    "        dy = box_trans[..., 1]\n",
    "        dw = box_trans[..., 2]\n",
    "        dh = box_trans[..., 3]\n",
    "\n",
    "        pred_center_x = dx*w[:, None] + center_x[:, None]\n",
    "        pred_w = torch.exp(dw) + w[:, None]\n",
    "        pred_center_y = dy*h[:, None] + center_y[:, None]\n",
    "        pred_h = torch.exp(dh) + h[:, None]\n",
    "\n",
    "        pred_box_x1 = pred_center_x - 0.5*pred_w\n",
    "        pred_box_y1 = pred_center_y - 0.5*pred_h\n",
    "        pred_box_x2 = pred_center_x + 0.5*pred_w\n",
    "        pred_box_y2 = pred_center_y + 0.5*pred_h\n",
    "\n",
    "        pred_boxes = torch.stack((\n",
    "            pred_box_x1,\n",
    "            pred_box_y1,\n",
    "            pred_box_x2,\n",
    "            pred_box_y2\n",
    "        ), dim=2)\n",
    "\n",
    "        return pred_boxes\n",
    "\n",
    "\n",
    "def get_iou(box1, box2):\n",
    "     \n",
    "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "\n",
    "    x_left = torch.max(box1[:, None, 0], box2[:, None, 0])\n",
    "    y_top = torch.max(box1[:, None, 1], box2[:, None, 1])\n",
    "\n",
    "    x_right = torch.min(box1[:, None, 2], box2[:, None, 2])\n",
    "    y_bottom = torch.min(box1[:, None, 3], box2[:, None, 3])\n",
    "\n",
    "    intersection_area = (x_right - x_left).clamp(min=0) * (y_bottom - y_top).clamp(min=0)\n",
    "    union_area = area1[:, None] + area2 - intersection_area\n",
    "\n",
    "    return intersection_area/union_area\n",
    "\n",
    "\n",
    "def clap_box_to_img_size(boxes, image_shape):\n",
    "    boxes_x1 = boxes[..., 0]\n",
    "    boxes_y1 = boxes[..., 1]\n",
    "    boxes_x2 = boxes[..., 2]\n",
    "    boxes_y2 = boxes[..., 3]\n",
    "    hieght, width = image_shape[-2:]\n",
    "\n",
    "    boxes_x1 = boxes_x1.clamp(0, width)\n",
    "    boxes_x2 = boxes_x2.clamp(0, width)\n",
    "    boxes_y1 = boxes_y1.clamp(0, hieght)\n",
    "    boxes_y2 = boxes_y2.clamp(0, hieght)\n",
    "\n",
    "    boxes = torch.cat((\n",
    "        boxes_x1[..., None],\n",
    "        boxes_y1[..., None],\n",
    "        boxes_x2[..., None],\n",
    "        boxes_y2[..., None]\n",
    "    ), dim=-1)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def boxes_to_transformation_targets(ground_truth_boxes, anchor_for_propoosals):\n",
    "    width = anchor_for_propoosals[:, 2] - anchor_for_propoosals[:, 0]\n",
    "    height = anchor_for_propoosals[:, 3] - anchor_for_propoosals[:, 1]\n",
    "    center_x = anchor_for_propoosals[:, 0] + 0.5*width\n",
    "    center_y = anchor_for_propoosals[:, 1] + 0.5*height\n",
    "\n",
    "    gt_width = ground_truth_boxes[:, 2] - ground_truth_boxes[:, 0]\n",
    "    gt_height = ground_truth_boxes[:, 3] - ground_truth_boxes[:, 1]\n",
    "    gt_center_x = ground_truth_boxes[:, 0] + 0.5*gt_width\n",
    "    gt_center_y = ground_truth_boxes[:, 1] + 0.5*gt_height\n",
    "\n",
    "    target_dx = (gt_center_x - center_x)/width\n",
    "    target_dy = (gt_center_y - center_y)/height\n",
    "    target_dw = torch.log(gt_width/width)\n",
    "    target_dh = torch.log(gt_height/height)\n",
    "\n",
    "    regression_targets = torch.stack((\n",
    "        target_dx,\n",
    "        target_dy,\n",
    "        target_dw,\n",
    "        target_dh\n",
    "    ), dim=1)\n",
    "\n",
    "    return regression_targets\n",
    "\n",
    "\n",
    "def transform_boxes_to_original_size(boxes, new_size, original_size):\n",
    "    r\"\"\"\n",
    "    Boxes are for resized image (min_size=600, max_size=1000).\n",
    "    This method converts the boxes to whatever dimensions\n",
    "    the image was before resizing\n",
    "    :param boxes:\n",
    "    :param new_size:\n",
    "    :param original_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ratios = [\n",
    "        torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
    "        / torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
    "        for s, s_orig in zip(new_size, original_size)\n",
    "    ]\n",
    "    ratio_height, ratio_width = ratios\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    xmin = xmin * ratio_width\n",
    "    xmax = xmax * ratio_width\n",
    "    ymin = ymin * ratio_height\n",
    "    ymax = ymax * ratio_height\n",
    "    return torch.stack((xmin, ymin, xmax, ymax), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "def sample_positive_negative(labels, positive_count, total_count):\n",
    "    positive = torch.where(labels>=1)[0]\n",
    "    negative = torch.where(labels==0)[0]\n",
    "\n",
    "    num_positive = positive_count\n",
    "    num_positive = torch.min(positive.numel(), num_positive)\n",
    "\n",
    "    num_negative = total_count - num_positive\n",
    "    num_negative = torch.min(negative.numel(), num_negative)\n",
    "\n",
    "    perm_pos_index = torch.randperm(\n",
    "        positive.numel(), device=positive.device\n",
    "    )[:num_positive]\n",
    "    perm_neg_index = torch.randperm(\n",
    "        negative.numel(), device=negative.device\n",
    "    )[:num_negative]\n",
    "\n",
    "    pos_index = positive[perm_pos_index]\n",
    "    neg_index = negative[perm_neg_index]\n",
    "\n",
    "    sampled_pos_index = torch.zeros_like(labels, dtype=torch.bool)\n",
    "    sampled_neg_index = torch.zeros_like(labels, dtype=torch.bool)\n",
    "    sampled_pos_index[pos_index] = True\n",
    "    sampled_neg_index[neg_index] = True\n",
    "\n",
    "    return sampled_neg_index, sampled_pos_index\n",
    "\n",
    "\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(self, in_channels = 512) -> None:\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.scales = [128, 256, 512]\n",
    "        self.aspect_ratio = [0.5, 1, 2]\n",
    "        self.num_anchors = len(self.scales)*len(self.aspect_ratio)\n",
    "\n",
    "\n",
    "        self.rpn_conv = nn.Conv2d(in_channels,\n",
    "                                  in_channels,\n",
    "                                  kernel_size=3,\n",
    "                                  stride=1,\n",
    "                                  padding=1\n",
    "                            )\n",
    "        self.clf_layer = nn.Conv2d(in_channels, \n",
    "                                   self.num_anchors,\n",
    "                                   kernel_size=1,\n",
    "                                   stride=1)\n",
    "        self.bbox_reg = nn.Conv2d(in_channels,\n",
    "                                  self.num_anchors*4,\n",
    "                                  kernel_size=1,\n",
    "                                  stride=1)\n",
    "        \n",
    "    def generate_anchors(self, image, feat):\n",
    "        grid_h, grid_w = feat.shape[-2:]\n",
    "        image_h, image_w = image.shape[-2:]\n",
    "        stride_h = torch.tensor(image_h//grid_h,\n",
    "                                dtype=torch.int64,\n",
    "                                device=feat.device)\n",
    "        stride_w = torch.tensor(image_w//grid_w,\n",
    "                                dtype=torch.int64,\n",
    "                                device=feat.device)\n",
    "        scales = torch.tensor(self.scales, \n",
    "                              dtype=feat.dtype,\n",
    "                              device=feat.device)\n",
    "        aspect_rotation = torch.tensor(self.aspect_ratio,\n",
    "                                       dtype=feat.dtype,\n",
    "                                       device=feat.device)\n",
    "        \n",
    "        h_rations = torch.sqrt(aspect_rotation)\n",
    "        w_rations = 1/h_rations\n",
    "\n",
    "        ws = (w_rations[:, None] * scales[None, :]).view(-1)\n",
    "        hs = (h_rations[:, None] * scales[None, :]).view(-1)\n",
    "        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1)/2\n",
    "        base_anchors = base_anchors.round()\n",
    "\n",
    "        shift_x = torch.arange(0, grid_w,\n",
    "                               dtype=torch.int64,\n",
    "                               device=feat.device) * stride_w\n",
    "        shift_y = torch.arange(0, grid_h,\n",
    "                               dtype=torch.int64,\n",
    "                               device=feat.device) * stride_h\n",
    "        shift_y, shift_x = torch.meshgrid(shift_y, shift_x, \n",
    "                                          indexing='ij')\n",
    "        \n",
    "        shift_x = shift_x.reshape(-1)\n",
    "        shift_y = shift_y.reshape(-1)\n",
    "        shifts = torch.stack((shift_x,\n",
    "                              shift_y,\n",
    "                              shift_x,\n",
    "                              shift_y), dim=1)\n",
    "        anchors = (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4))\n",
    "        anchors = anchors.reshape(-1, 4)\n",
    "        return anchors\n",
    "    \n",
    "\n",
    "    def apply_regressions_pred_to_anchors_or_proposals(box_trans, anchor_or_prop):\n",
    "        box_trans = box_trans.reshape(\n",
    "            box_trans.size(0), -1, 4\n",
    "        )\n",
    "        w = anchor_or_prop[:, 2] - anchor_or_prop[:, 0]\n",
    "        h = anchor_or_prop[:, 3] - anchor_or_prop[:, 1]\n",
    "        center_x = anchor_or_prop[:, 0] + 0.5*w\n",
    "        center_y = anchor_or_prop[:, 1] + 0.5*h\n",
    "\n",
    "        dx = box_trans[..., 0]\n",
    "        dy = box_trans[..., 1]\n",
    "        dw = box_trans[..., 2]\n",
    "        dh = box_trans[..., 3]\n",
    "\n",
    "        pred_center_x = dx*w[:, None] + center_x[:, None]\n",
    "        pred_w = torch.exp(dw) + w[:, None]\n",
    "        pred_center_y = dy*h[:, None] + center_y[:, None]\n",
    "        pred_h = torch.exp(dh) + h[:, None]\n",
    "\n",
    "        pred_box_x1 = pred_center_x - 0.5*pred_w\n",
    "        pred_box_y1 = pred_center_y - 0.5*pred_h\n",
    "        pred_box_x2 = pred_center_x + 0.5*pred_w\n",
    "        pred_box_y2 = pred_center_y + 0.5*pred_h\n",
    "\n",
    "        pred_boxes = torch.stack((\n",
    "            pred_box_x1,\n",
    "            pred_box_y1,\n",
    "            pred_box_x2,\n",
    "            pred_box_y2\n",
    "        ), dim=2)\n",
    "\n",
    "        return pred_boxes\n",
    "    \n",
    "\n",
    "    def assign_target_to_anchors(self, anchors, gt_boxes):\n",
    "        iou_matrix = get_iou(gt_boxes, anchors)\n",
    "        best_match_iou, index = iou_matrix.max(dim=0)\n",
    "\n",
    "        best_match_iou_index_copy = index.clone()\n",
    "\n",
    "        below_low = best_match_iou < 0.3\n",
    "        between = (best_match_iou > 0.3) & (best_match_iou < 0.7)\n",
    "\n",
    "        index[below_low] = -1\n",
    "        index[between] = -2\n",
    "\n",
    "        best_anchor_for_gt, _ = iou_matrix.max(dim=1)\n",
    "        gt_pred_pairs_with_highst_iou = torch.where(iou_matrix==best_anchor_for_gt[:, None])\n",
    "        pred_inds_to_update = gt_pred_pairs_with_highst_iou[1]\n",
    "        best_match_iou[pred_inds_to_update] = best_match_iou_index_copy[pred_inds_to_update]\n",
    "\n",
    "        matched_gt_boxes = gt_boxes[best_match_iou.clamp(min=0)]\n",
    "        labels = best_match_iou >= 0\n",
    "        labels = labels.to(dtype=torch.float32)\n",
    "\n",
    "        backgroud_anchors = best_match_iou == -1\n",
    "        labels[backgroud_anchors] = 0.0\n",
    "\n",
    "        ignored_anchores = best_match_iou == -2\n",
    "        labels[ignored_anchores] = -1.0\n",
    "        \n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    \n",
    "\n",
    "    def filter_propoosals(self, propoosels, clf_score, img_shape):\n",
    "        clf_score = clf_score.reshape(-1)\n",
    "        clf_score = torch.sigmoid(clf_score)\n",
    "        _, top_in_index = clf_score.topk(1000)\n",
    "        clf_score = clf_score[top_in_index]\n",
    "        propoosels = propoosels[top_in_index]\n",
    "\n",
    "        propoosels = clap_box_to_img_size(propoosels, img_shape)\n",
    "\n",
    "        keep_mask = torch.zeros_like(clf_score, dtype=bool)\n",
    "        keep_indices = torch.ops.torchvision.nms(propoosels,\n",
    "                                                 clf_score, 0.7)\n",
    "        \n",
    "        post_nms_keep_indecies = keep_indices[\n",
    "                                clf_score[keep_indices].sort(descending=True)[1]\n",
    "                                            ]\n",
    "        \n",
    "        propoosels = propoosels[post_nms_keep_indecies[:2000]]\n",
    "        clf_score = clf_score[post_nms_keep_indecies[:2000]]\n",
    "\n",
    "        return propoosels, clf_score\n",
    "         \n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, image, feat, target):\n",
    "        rpn_feat = nn.ReLU()(self.rpn_conv(feat))\n",
    "        cls = self.clf_layer(rpn_feat)\n",
    "        box_trans = self.bbox_reg(rpn_feat)\n",
    "        anchors = self.generate_anchors(image, feat)\n",
    "        number_of_anchors_per_location = cls.size(1)\n",
    "        cls = cls.permute(0, 2, 3, 1)\n",
    "        cls = cls.reshape(-1, 1)\n",
    "        box_trans = box_trans.view(\n",
    "            box_trans.size(0),\n",
    "            number_of_anchors_per_location,\n",
    "            4,\n",
    "            rpn_feat.shape[-1],\n",
    "            rpn_feat.shape[-2]\n",
    "        )\n",
    "        box_trans = box_trans.permute(0, 3, 4, 1, 2)\n",
    "        box_trans = box_trans.reshape(-1, 4)\n",
    "\n",
    "        propoosels = apply_regressions_pred_to_anchors_or_proposals(\n",
    "            box_trans.detach().reshape(-1, 1, 4),\n",
    "            anchors)\n",
    "        propoosels = propoosels.reshape(propoosels.size(0), 4)\n",
    "        propoosels, scores = self.filter_propoosals(propoosels,\n",
    "                                                    cls.detach(),\n",
    "                                                    image.shape)\n",
    "        rpn_output = {\n",
    "            'propoosels': propoosels,\n",
    "            'scores': scores\n",
    "        }\n",
    "\n",
    "        if not self.training or target is None:\n",
    "            return rpn_output\n",
    "        else:\n",
    "            labels_for_anchors, matched_gt_boxes_for_anchors = self.assign_target_to_anchors(\n",
    "                anchors,\n",
    "                target['bboxes'][0]\n",
    "            )\n",
    "\n",
    "            regression_targets = boxes_to_transformation_targets(\n",
    "                matched_gt_boxes_for_anchors, anchors\n",
    "            )\n",
    "            sampled_neg_mask, sampled_pos_mask = sample_positive_negative(\n",
    "                labels_for_anchors, positive_count=128, total_count=256\n",
    "                )\n",
    "            \n",
    "            sampled_index = torch.where(sampled_pos_mask | sampled_neg_mask)[0]\n",
    "            localization_loss = (\n",
    "                nn.functional.smooth_l1_loss(\n",
    "                    box_trans[sampled_pos_mask],\n",
    "                    regression_targets[sampled_pos_mask],\n",
    "                    beta = 1/9,\n",
    "                    reuction = 'sum'\n",
    "                )/(sampled_index.numel())\n",
    "            )\n",
    "            cls_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "                cls[sampled_index].flatten(),\n",
    "                labels_for_anchors[sampled_index].flatten()\n",
    "            )\n",
    "            rpn_output['cls_loss'] = cls_loss\n",
    "            rpn_output['localization_loss'] = localization_loss\n",
    "\n",
    "            return rpn_output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROIHead(nn.Module):\n",
    "    def __init__(self, num_classes=2, in_channels = 512):\n",
    "        super(ROIHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.pool_size = 7\n",
    "        self.fc_inner_dim = 1024\n",
    "\n",
    "        self.fc6 = nn.Linear(in_channels*self.pool_size*self.pool_size,\n",
    "                              self.fc_inner_dim)\n",
    "        self.fc7 = nn.Linear(self.fc_inner_dim,\n",
    "                             self.fc_inner_dim)\n",
    "        self.fccls = nn.Linear(self.fc_inner_dim,\n",
    "                               self.num_classes)\n",
    "        self.reg_layer = nn.Linear(self.fc_inner_dim,\n",
    "                                   self.num_classes*4)\n",
    "        \n",
    "\n",
    "    def assign_target_to_propoosels(self, propoosels, gt_boxes, gt_labels):\n",
    "        iou_metrix = get_iou(gt_boxes, propoosels)\n",
    "        best_match_iou, best_match_gt_index = iou_metrix.max(dim=0)\n",
    "        below_low = best_match_iou < 0.5\n",
    "\n",
    "        best_match_gt_index[below_low] = -1\n",
    "        matched_gt_boxes_for_propoosels = gt_boxes[best_match_gt_index.clamp(min=0)]\n",
    "\n",
    "        labels = gt_labels[best_match_gt_index.clamp(min=0)]\n",
    "        labels = labels.to(dtype=torch.int64)\n",
    "\n",
    "        background_propoosels = best_match_gt_index == -1\n",
    "        labels[background_propoosels] = 0\n",
    "\n",
    "        return labels, matched_gt_boxes_for_propoosels\n",
    "    \n",
    "\n",
    "\n",
    "    def filter_predictions(self, pred_box, pred_labels, pred_scores):\n",
    "        keep = torch.where(pred_scores > 0.05)[0]\n",
    "        pred_box, pred_scores, pred_labels = pred_box[keep], pred_scores[keep], pred_labels[keep]\n",
    "\n",
    "        min_size = 1\n",
    "        ws, hs = pred_box[:, 2] - pred_box[:, 0], pred_box[:, 3] - pred_box[:, 1]\n",
    "        keep = (ws >= min_size) & (hs >= min_size)\n",
    "        keep = torch.where(keep)[0]\n",
    "        pred_box, pred_scores, pred_labels = pred_box[keep], pred_scores[keep], pred_labels[keep]\n",
    "\n",
    "        keep_mask = torch.zeros_like(pred_scores, dtype=torch.bool)\n",
    "        for class_id in torch.unique(pred_labels):\n",
    "            curr_indices = torch.where(pred_labels == class_id)[0]\n",
    "            curr_keep_indices = torchvision.ops.nms(\n",
    "                pred_box[curr_indices],\n",
    "                pred_scores[curr_indices],\n",
    "                0.5\n",
    "            )\n",
    "            keep_mask[curr_indices[curr_keep_indices]] = True\n",
    "        keep_indecies = torch.where(keep_mask)[0]\n",
    "        post_nms_keep_indecies = keep_indecies[pred_scores[keep_indecies].sort(\n",
    "            descending=True\n",
    "        )[1]]\n",
    "        keep = post_nms_keep_indecies[:100]\n",
    "        pred_box, pred_scores, pred_labels = pred_box[keep], pred_scores[keep], pred_labels[keep]\n",
    "\n",
    "        return pred_box, pred_scores, pred_labels\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, feat, propoosals, img_shape, target):\n",
    "        if self.training and target is not None:\n",
    "            gt_boxes = target['bbox'][0]\n",
    "            gt_labels = target['labels'][0]\n",
    "            labels, matched_gt_boxes_for_propoosels = self.assign_target_to_propoosels(\n",
    "                propoosals, gt_boxes, gt_labels\n",
    "            )\n",
    "            sampled_neg_indx_mask, sampled_pos_indx_mask = sample_positive_negative(\n",
    "                labels, positive_count=32, total_count=128\n",
    "            )\n",
    "            sampled_indx = torch.where(sampled_pos_indx_mask | sampled_neg_indx_mask)[0]\n",
    "            propoosals = propoosals[sampled_indx]\n",
    "            labels = labels[sampled_indx]\n",
    "\n",
    "            matched_gt_boxes_for_propoosels = matched_gt_boxes_for_propoosels[sampled_indx]\n",
    "            regrassion_targets = boxes_to_transformation_targets(\n",
    "                matched_gt_boxes_for_propoosels, propoosals\n",
    "            )\n",
    "\n",
    "        spatial_scale = 0.0625\n",
    "\n",
    "        propoosal_roi_pool_feats = torchvision.ops_roi_pool(\n",
    "            feat,\n",
    "            [propoosals],\n",
    "            output_size = self.pool_size,\n",
    "            spatial_scale = spatial_scale\n",
    "        \n",
    "        )\n",
    "        propoosal_roi_pool_feats = propoosal_roi_pool_feats.flatten(start_dim=1)\n",
    "        box_fc6 = torch.nn.functional.relu(self.fc6(propoosal_roi_pool_feats))\n",
    "        box_fc7 = torch.nn.functiomal.relu(self.fc7(box_fc6))\n",
    "        cls_score = self.fccls(box_fc7)\n",
    "        box_transform_pred = self.reg_layer(box_fc7)\n",
    "\n",
    "        num_boxes, num_classes = cls_score.shape\n",
    "        box_transform_pred = box_transform_pred.reshape(num_boxes, num_classes, 4)\n",
    "\n",
    "        frcnn_output = {}\n",
    "        if self.training and target is not None:\n",
    "            classification_loss = nn.functional.cross_entropy(\n",
    "                cls_score,\n",
    "                labels\n",
    "            )\n",
    "\n",
    "            fg_propoosal_indx = torch.where(labels > 0)[0]\n",
    "            fg_class_labels = labels[fg_propoosal_indx]\n",
    "            localization_loss = torch.nn.functional.smooth_l1_loss(\n",
    "                box_transform_pred[fg_propoosal_indx, fg_class_labels],\n",
    "                regrassion_targets[fg_propoosal_indx],\n",
    "                beta=1/9,\n",
    "                reduction = 'sum'\n",
    "            )\n",
    "            localization_loss = localization_loss/labels.numel()\n",
    "            frcnn_output['frcnn-classification-loss'] = classification_loss\n",
    "            frcnn_output['frcnn-localiztion-loss'] = localization_loss\n",
    "            return frcnn_output\n",
    "        \n",
    "        else:\n",
    "            pred_boxes = apply_regressions_pred_to_anchors_or_proposals(\n",
    "                box_transform_pred,\n",
    "                propoosals\n",
    "            )\n",
    "            pred_scores = torch.nn.functional.softmax(cls_score, dim=-1)\n",
    "\n",
    "            pred_boxes = clap_box_to_img_size(pred_boxes, img_shape)\n",
    "            pred_labels = torch.arange(num_classes, device=cls_score.device)\n",
    "            pred_labels = pred_labels.view(1, -1).expand_as(pred_scores)\n",
    "\n",
    "            pred_boxes = pred_boxes[:, 1:]\n",
    "            pred_labels = pred_labels[:, 1:]\n",
    "            pred_scores = pred_scores[:, 1:]\n",
    "\n",
    "            pred_boxes = pred_boxes.reshape(-1, 4)\n",
    "            pred_labels = pred_labels.reshape(-1)\n",
    "            pred_scores = pred_scores.reshape(-1)\n",
    "\n",
    "            pred_boxes, pred_labels, pred_scores = self.filter_predictions(\n",
    "                pred_boxes,\n",
    "                pred_labels,\n",
    "                pred_scores\n",
    "            )\n",
    "            frcnn_output['bboxes'] = pred_boxes\n",
    "            frcnn_output['scores'] = pred_scores\n",
    "            frcnn_output['labels'] = pred_labels\n",
    "\n",
    "            return frcnn_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2) -> None:\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "        self.backbone = vgg16.features[:-1]\n",
    "        self.rpn = RegionProposalNetwork(in_channels=512)\n",
    "        self.roihead = ROIHead(num_classes=num_classes,\n",
    "                               in_channels=512)\n",
    "        for layer in self.backbone[:10]:\n",
    "            for p in layer.parameters():\n",
    "                p.requiers_grad = False\n",
    "        self.image_mean = [0.485, 0.456, 0.406]\n",
    "        self.image_std = [0.229, 0.224, 0.225]\n",
    "        self.min_size = 600\n",
    "        self.max_size = 1000\n",
    "\n",
    "    def normalize_resize_image_and_boxes(self, image, bboxes):\n",
    "        mean = torch.as_tensor(self.image_mean,\n",
    "                               dtype=image.dtype,\n",
    "                               device=image.device)\n",
    "        std = torch.as_tensor(self.image_std,\n",
    "                              dtype=image.dtype,\n",
    "                              device=image.device)\n",
    "        image = (image - mean[:, None, None]) / std[:, None, None]\n",
    "\n",
    "        h, w = image.shape[-2:]\n",
    "        im_shape = torch.tensor(image.shape[-2:])\n",
    "        min_size = torch.min(im_shape).to(dtype=torch.float32)\n",
    "        max_size = torch.max(im_shape).to(dtype=torch.float32)\n",
    "        scale = torch.min(\n",
    "            float(self.min_size) / min_size,\n",
    "            float(self.max_size) / max_size\n",
    "        )\n",
    "\n",
    "        scale_factor = scale.item()\n",
    "        image = torch.nn.functional.interpolate(\n",
    "            image,\n",
    "            size=None,\n",
    "            scale_factor=scale_factor,\n",
    "            mode='bilinear',\n",
    "            recompute_scale_factor=True,\n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        if bboxes is not None:\n",
    "            ratios = [\n",
    "                torch.tensor(s, dtype=torch.float32, device=bboxes.device)\n",
    "                /torch.tensor(s_orig, dtype=torch.float32, device=bboxes.device)\n",
    "                for s, s_orig in zip(image.shape[-2:], (h, w))\n",
    "            ]\n",
    "            ratio_height, ratio_width = ratios\n",
    "            xmin, ymin, xmax, ymax = bboxes.unbind(2)\n",
    "            xmin = xmin + ratio_width\n",
    "            ymin = ymin + ratio_height\n",
    "            xmax = xmax + ratio_width\n",
    "            ymax = ymax + ratio_height\n",
    "            bboxes = torch.stack((\n",
    "                xmin,\n",
    "                ymin,\n",
    "                xmax,\n",
    "                ymax\n",
    "            ), dim=2)\n",
    "\n",
    "            return image, bboxes\n",
    "    \n",
    "\n",
    "    def forward(self, image, target=None):\n",
    "        old_shape = image.shape[-2:]\n",
    "        if self.training:\n",
    "            image, bboxes = self.normalize_resize_image_and_boxes(\n",
    "                image,\n",
    "                target['bboxes']\n",
    "            )\n",
    "            target['bboxes'] = bboxes\n",
    "        else:\n",
    "            image, _ = self.normalize_resize_image_and_boxes(\n",
    "                image,\n",
    "                None\n",
    "            )\n",
    "        feat = self.backbone(image)\n",
    "        rpn_output = self.rpn(image, feat, target)\n",
    "        propoosals = rpn_output['propoosals']\n",
    "        frcnn_out = self.roihead(feat, propoosals, image.shape[-2:])\n",
    "\n",
    "        if not self.training:\n",
    "            frcnn_out['boxes'] = transform_boxes_to_original_size(\n",
    "                frcnn_out['boes'],\n",
    "                image.shape[-2:],\n",
    "                old_shape\n",
    "            )\n",
    "        return rpn_output, frcnn_out\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
